{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Profile Creation Model Package from AWS Marketplace \n",
    "\n",
    "\n",
    "An NLP and knowledge graph based approach to create company profile by aggregating data from multiple sources.\n",
    "\n",
    "This solution creates a knowledge graph based on entity-name pairs from data collected from multiple sources of information such as Wikipedia, company's website, CrunchBase etc. This solution creates a graph model of a company's profile based on unstructured data.\n",
    "\n",
    "This sample notebook shows you how to deploy Company Profile Creation.\n",
    "\n",
    "\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to Company Profile Creation. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page **Company Profile Creation.**\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper code to crawl the data from the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save(site, filePath):\n",
    "    \"\"\"\n",
    "    Reads given URL, processes it, and saves resulting JSON object to\n",
    "    filePath + /page.json returns the a tags extracted from a-tags\n",
    "    Input:  site = URL to website\n",
    "            filePath = filePath to directory to save page.json file in\n",
    "    Output: returns array of tags extracted\n",
    "    \"\"\"\n",
    "\n",
    "    #Code for creating pathway to page.json file if one does not exist\n",
    "    path = filePath.split(\"/\")\n",
    "    curr = \"\"\n",
    "    while (len(path)):\n",
    "        curr += (\"/\" if curr != \"\" else curr) + path.pop(0)\n",
    "        try:\n",
    "            os.mkdir(curr)\n",
    "        except:\n",
    "            pass\n",
    "    data = {}\n",
    "\n",
    "    #Gets website data, otherwise returns 3 and saves error message\n",
    "    try:\n",
    "\n",
    "        site = str(urlopen(site).read())\n",
    "        #--------entry point OF html----------------#\n",
    "        \n",
    "        if '<html' not in site:\n",
    "            print(\"Not an html site\")\n",
    "            site = \"<xYnC>failed</xYnC>\"\n",
    "        else:\n",
    "            aTags = re.findall(r'<a.*?href=[\\\"|\\'](/.*?)[\\\"|\\']', site)\n",
    "    except:\n",
    "        print(\"Invalid URL or failed to access: \" + site)\n",
    "        site = \"<xYnC>failed</xYnC>\"\n",
    "    if (site == \"<xYnC>failed</xYnC>\"):\n",
    "        data[\"error\"]=\"Invalid URL or failed to access\"\n",
    "        return 3\n",
    "    #processing for the site data\n",
    "    else:\n",
    "        #Remove head\n",
    "        site = re.sub(r'<head.*?>.*?</head>', '', site, 1)\n",
    "        try:\n",
    "            #tags that we are going to pick out\n",
    "            tags = [r'h1', r'h2', r'h3', r'p']\n",
    "            #creates the stack that we will be using\n",
    "            queue = [(0, site, data)]\n",
    "            #runs the code till we create the full dict structure\n",
    "            while queue:\n",
    "                #pops elem from stack and processes the components\n",
    "                pog = queue.pop(-1)\n",
    "                tagIndex = pog[0]\n",
    "                site = pog[1]\n",
    "                d = pog[2]\n",
    "                #handles the p-tag case (captures li and a-tag text as well\n",
    "                if tagIndex == len(tags) - 1:\n",
    "                    pUnf = [re.sub(r'(?:<.*?>|\\\\+(?:[A-Za-mo-rt-z0-9]+| |\\'))', '', x).strip()\n",
    "                                                  for x in re.findall(r'<(?:[pa]|li).*?>(.*?)<(?:/p>|/a>|/li>|(?:h1|h2|h3).*?>)', site)]\n",
    "                    d[\"p\"] = pUnf\n",
    "                #Captures given headers and assigns content to it\n",
    "                else:\n",
    "                    #finding headers\n",
    "                    titles = [re.sub(r'(?:<.*?>|\\\\+(?:[A-Za-mo-rt-z0-9]+| |\\'))', '', x).strip()\n",
    "                                          for x in re.findall(r'<' + tags[tagIndex] + r'.*?>(.*?)</(?:' +\n",
    "                                                              tags[tagIndex] + r'|div)>', site)]\n",
    "                    #if headers exist, we assign content and push to stack\n",
    "                    if titles:\n",
    "                        x = re.search(r'(.*?)<' + tags[tagIndex] + r'.*?>', site).group(0)\n",
    "                        if x:\n",
    "                            queue.append((tagIndex + 1, x, d))\n",
    "                        contents = [x for x in re.findall(r'</' + tags[tagIndex] + r'>.*?<(?:' + tags[tagIndex] + r'|div).*?>', site)]\n",
    "                        site = re.sub(r'</' + tags[tagIndex] + r'>.*?<(?:' + tags[tagIndex] + r'|div).*?>', '', site)\n",
    "                        contents.append(site)\n",
    "                        for x in range(len(titles)):\n",
    "                            d[titles[x]] = {}\n",
    "                            queue.append((tagIndex + 1, contents[x], d[titles[x]]))\n",
    "                            queue.append((tagIndex + 1, site, d))\n",
    "                    # if headers dont exist of this kind we move onto the next kind\n",
    "                    else:\n",
    "                        queue.append((tagIndex + 1, site, d))\n",
    "            #saves the data to the fp (file path)\n",
    "            with open(filePath + '/page.json', 'w') as fp:\n",
    "                json.dump(data, fp)\n",
    "        except Exception as e:\n",
    "            data[\"error\"] = \"Did not finish\"\n",
    "            with open(filePath + '/page.json', 'w') as fp:\n",
    "                json.dump(data, fp)\n",
    "            print(e)\n",
    "        return aTags\n",
    "\n",
    "\n",
    "def crawl(landing, depth=3, baseDir=\"data\", lander=\"\", thorough=True):\n",
    "    \"\"\"\n",
    "    Performs a breadth first traversal on given website and saves the\n",
    "    processed websites using the save() function in baseDir\n",
    "    Input:  landing = URL to website (make sure nothing after .com, .edu etc\n",
    "            depth = depth of the traversal\n",
    "            baseDir (optional) = the place where you want to store the folder\n",
    "                                    crawl will create\n",
    "            lander (optional) = limits the data saved to the urls containing\n",
    "                                    landing + lander\n",
    "    \"\"\"\n",
    "\n",
    "    #Standardized inputs\n",
    "    while landing[-1] == \"/\":\n",
    "        landing = landing[0:-1]\n",
    "    while lander != \"\" and lander[-1] == \"/\":\n",
    "        lander = lander[0:-1]\n",
    "    #Setting directory base\n",
    "    inDir = baseDir + \"/\"\n",
    "    try:\n",
    "        os.mkdir(inDir)\n",
    "    except:\n",
    "        pass\n",
    "    site = landing + lander\n",
    "    queue = [(0, site)]\n",
    "    #keeping track of failed visits\n",
    "    revisit = []\n",
    "    #keeping track of visits to stop repeats\n",
    "    visited = []\n",
    "    #works till queue is not empty\n",
    "    while (len(queue)):\n",
    "        #takes first elem in queue and then standardizes\n",
    "        curr = queue.pop(0)\n",
    "        print(curr)\n",
    "        if landing not in curr[1]:\n",
    "            site = landing + curr[1]\n",
    "        else:\n",
    "            site = curr[1]\n",
    "        #processing the site\n",
    "        if site not in visited:\n",
    "            print(site)\n",
    "            visited.append(site)\n",
    "            #processes site only if lander in site tag\n",
    "            if lander in site:\n",
    "                ind = site.rindex(lander) + len(lander)\n",
    "                if (ind == len(site) or site[ind] == \"/\"):\n",
    "                    tags = save(site, inDir + re.findall(r'https?:/+(.*)', site)[0])\n",
    "                elif thorough:\n",
    "                    try:\n",
    "                        tags = re.findall(r'<a.*?href=[\\\"|\\'](/.*?)[\\\"|\\']', site)\n",
    "                    except:\n",
    "                        print(\"Invalid URL or failed to access: \" + site)\n",
    "                        revisit.append(site)\n",
    "                        tags = []\n",
    "            # if thorough gets the a-tags and doesnt save if lander not in tag\n",
    "            elif thorough:\n",
    "                try:\n",
    "                    tags = re.findall(r'<a.*?href=[\\\"|\\'](/.*?)[\\\"|\\']', site)\n",
    "                except:\n",
    "                    print(\"Invalid URL or failed to access: \" + site)\n",
    "                    revisit.append(site)\n",
    "                    tags = []\n",
    "            else:\n",
    "                tags = []\n",
    "            # Adds a-tags that are found to the queue\n",
    "            if curr[0] < depth and tags != 3:\n",
    "                for tag in tags:\n",
    "                    q = re.findall(landing + r'(.*)', tag)\n",
    "                    if (not q or \".\" not in q[0] or \".html\" in tag) and tag not in visited:\n",
    "                        if \"#\" in tag:\n",
    "                            tag = tag[:tag.index(\"#\")]\n",
    "                        if landing not in tag:\n",
    "                            site = landing + tag\n",
    "                        else:\n",
    "                            site = tag\n",
    "                        while site != \"\" and site[-1] == \"/\":\n",
    "                            site = site[:-1]\n",
    "                        if site not in visited:\n",
    "                            queue.append((curr[0] + 1, site))\n",
    "    #Revisites all failed sites to see if they work the second time around\n",
    "    if len(revisit) > 0:\n",
    "        print(\"\\n\\n\\n\\n\\n\\n\\nRetrying.....\\n\")\n",
    "        for tag in revisit:\n",
    "            print(tag)\n",
    "            try:\n",
    "                site = str(urlopen(landing + tag).read())\n",
    "                if '<html' not in site:\n",
    "                    print(\"Not an HTML file\")\n",
    "                    site = \"<xYnC>failed</xYnC>\"\n",
    "            except:\n",
    "                print(\"Invalid URL or failed to access\")\n",
    "                site = \"<xYnC>failed</xYnC>\"\n",
    "            save(site, inDir + tag)\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n\\n\\nNothing to Retry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'https://jupiter.money')\n",
      "https://jupiter.money\n",
      "(1, 'https://jupiter.money/about')\n",
      "https://jupiter.money/about\n",
      "(1, 'https://jupiter.money/contact')\n",
      "https://jupiter.money/contact\n",
      "(1, 'https://jupiter.money/about')\n",
      "(2, 'https://jupiter.money/contact')\n",
      "(2, 'https://jupiter.money/contact')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nothing to Retry\n"
     ]
    }
   ],
   "source": [
    "crawl(\"https://jupiter.money/\", baseDir=\"Input\", thorough=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zipObj = zipfile.ZipFile('input.zip', 'w')\n",
    "zipObj.write('Input')\n",
    "zipObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = 'arn:aws:sagemaker:us-east-2:786796469737:model-package/companyportfolio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "#from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "#import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type='application/zip'\n",
    "model_name='company-port-folio'\n",
    "real_time_inference_instance_type='ml.m5.xlarge'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "#Define predictor wrapper class\n",
    "def predict_wrapper(endpoint, session):\n",
    "    return sage.RealTimePredictor(endpoint, session,content_type=content_type)\n",
    "#create a deployable model from the model package.\n",
    "model = ModelPackage(role=role,\n",
    "                    model_package_arn=model_package_arn,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    predictor_cls=predict_wrapper)\n",
    "\n",
    "#Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"input.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\", \r\n",
      "    \"ContentType\": \"application/zip\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name $model_name --body fileb://$file_name --content-type 'application/zip' --region us-east-2 output.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('output.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graphs/final_graph.gml') as f:\n",
    "    data = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph [\\n  node [\\n    id 0\\n    label \"juno\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 1\\n    label \"doggo\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 2\\n    label \"kedar nimkar\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 3\\n    label \"vp\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 4\\n    label \"kedar\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 5\\n    label \"finance\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 6\\n    label \"piyush kabra\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 7\\n    label \"piyush\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 8\\n    label \"nihar gupta\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 9\\n    label \"banking\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 10\\n    label \"nihar\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 11\\n    label \"jayesh sidhwani\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 12\\n    label \"jayeshs\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 13\\n    label \"data science\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 14\\n    label \"vivek ys\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 15\\n    label \"star trek\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 16\\n    label \"economics\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 17\\n    label \"statistics\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 18\\n    label \"venkat\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 19\\n    label \"cto\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 20\\n    label \"salil datar\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 21\\n    label \"coo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 22\\n    label \"anupam bagchi\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 23\\n    label \"cbo\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 24\\n    label \"anupam\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 25\\n    label \"jitendra gupta\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 26\\n    label \"ceo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 27\\n    label \"citrus pays\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 28\\n    label \"chief executive officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 29\\n    label \"chief operating officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 30\\n    label \"chief financial officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 31\\n    label \"cfo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 32\\n    label \"chief marketing officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 33\\n    label \"cmo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 34\\n    label \"chief technology officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 35\\n    label \"k\"\\n    data \"e\"\\n  ]\\n  node [\\n    id 36\\n    label \"n\"\\n    data \"i\"\\n  ]\\n  node [\\n    id 37\\n    label \"a\"\\n    data \"n\"\\n  ]\\n  edge [\\n    source 0\\n    target 1\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 2\\n    target 3\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 2\\n    target 4\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 3\\n    target 5\\n    weight 17.5\\n  ]\\n  edge [\\n    source 3\\n    target 6\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 3\\n    target 13\\n    weight 17.5\\n  ]\\n  edge [\\n    source 3\\n    target 14\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 5\\n    target 6\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 6\\n    target 7\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 8\\n    target 9\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 8\\n    target 10\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 11\\n    target 12\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 13\\n    target 14\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 14\\n    target 15\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 16\\n    target 17\\n    weight 17.5\\n  ]\\n  edge [\\n    source 16\\n    target 18\\n    weight 17.5\\n  ]\\n  edge [\\n    source 16\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 17\\n    target 18\\n    weight 17.5\\n  ]\\n  edge [\\n    source 17\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 18\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 19\\n    target 34\\n    weight 0\\n  ]\\n  edge [\\n    source 20\\n    target 21\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 21\\n    target 29\\n    weight 0\\n  ]\\n  edge [\\n    source 22\\n    target 23\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 22\\n    target 24\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 25\\n    target 26\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 25\\n    target 27\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 26\\n    target 28\\n    weight 0\\n  ]\\n  edge [\\n    source 30\\n    target 31\\n    weight 0\\n  ]\\n  edge [\\n    source 32\\n    target 33\\n    weight 0\\n  ]\\n  edge [\\n    source 35\\n    target 35\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 36\\n    target 36\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 37\\n    target 37\\n    weight 3.333333333333333\\n  ]\\n]\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34mInitializing NLP Library...\u001b[0m\n",
      "\u001b[34mInitialized NLP Library!\n",
      " * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\u001b[0m\n",
      "\u001b[34mInitializing NLP Library...\u001b[0m\n",
      "\u001b[34mInitialized NLP Library!\n",
      " * Debugger is active!\n",
      " * Debugger PIN: 266-560-827\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2021 15:39:33] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2021 15:39:33] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[34mcreated folder\u001b[0m\n",
      "\u001b[34mhere\u001b[0m\n",
      "\u001b[34mzip complete\u001b[0m\n",
      "\u001b[34mfinish\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2021 15:39:33] \"#033[37mPOST /invocations HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[32m2021-03-11T15:39:33.236:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "Batch Transform complete\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import uuid\n",
    "\n",
    "\n",
    "transformer = model.transformer(1, 'ml.m5.4xlarge')\n",
    "transformer.transform('s3://mphasis-marketplace/company_portfolio/input/Input.zip', content_type='application/zip')\n",
    "transformer.wait()\n",
    "#transformer.output_path\n",
    "print(\"Batch Transform complete\")\n",
    "bucketFolder = transformer.output_path.rsplit('/')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "#print(s3bucket,s3prefix)\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "bucket_name=\"sagemaker-us-east-2-786796469737\"\n",
    "with open('output.json', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name, bucketFolder+'/Input.zip.out', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('output.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph [\\n  node [\\n    id 0\\n    label \"juno\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 1\\n    label \"doggo\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 2\\n    label \"kedar nimkar\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 3\\n    label \"vp\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 4\\n    label \"kedar\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 5\\n    label \"finance\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 6\\n    label \"piyush kabra\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 7\\n    label \"piyush\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 8\\n    label \"nihar gupta\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 9\\n    label \"banking\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 10\\n    label \"nihar\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 11\\n    label \"jayesh sidhwani\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 12\\n    label \"jayeshs\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 13\\n    label \"data science\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 14\\n    label \"vivek ys\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 15\\n    label \"star trek\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 16\\n    label \"economics\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 17\\n    label \"statistics\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 18\\n    label \"venkat\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 19\\n    label \"cto\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 20\\n    label \"salil datar\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 21\\n    label \"coo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 22\\n    label \"anupam bagchi\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 23\\n    label \"cbo\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 24\\n    label \"anupam\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 25\\n    label \"jitendra gupta\"\\n    data \"_networkx_list_start\"\\n    data \"PERSON\"\\n  ]\\n  node [\\n    id 26\\n    label \"ceo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 27\\n    label \"citrus pays\"\\n    data \"_networkx_list_start\"\\n    data \"ORG\"\\n  ]\\n  node [\\n    id 28\\n    label \"chief executive officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 29\\n    label \"chief operating officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 30\\n    label \"chief financial officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 31\\n    label \"cfo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 32\\n    label \"chief marketing officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 33\\n    label \"cmo\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 34\\n    label \"chief technology officer\"\\n    data \"BIZ\"\\n  ]\\n  node [\\n    id 35\\n    label \"k\"\\n    data \"e\"\\n  ]\\n  node [\\n    id 36\\n    label \"n\"\\n    data \"i\"\\n  ]\\n  node [\\n    id 37\\n    label \"a\"\\n    data \"n\"\\n  ]\\n  edge [\\n    source 0\\n    target 1\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 2\\n    target 3\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 2\\n    target 4\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 3\\n    target 5\\n    weight 17.5\\n  ]\\n  edge [\\n    source 3\\n    target 6\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 3\\n    target 13\\n    weight 17.5\\n  ]\\n  edge [\\n    source 3\\n    target 14\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 5\\n    target 6\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 6\\n    target 7\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 8\\n    target 9\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 8\\n    target 10\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 11\\n    target 12\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 13\\n    target 14\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 14\\n    target 15\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 16\\n    target 17\\n    weight 17.5\\n  ]\\n  edge [\\n    source 16\\n    target 18\\n    weight 17.5\\n  ]\\n  edge [\\n    source 16\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 17\\n    target 18\\n    weight 17.5\\n  ]\\n  edge [\\n    source 17\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 18\\n    target 19\\n    weight 17.5\\n  ]\\n  edge [\\n    source 19\\n    target 34\\n    weight 0\\n  ]\\n  edge [\\n    source 20\\n    target 21\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 21\\n    target 29\\n    weight 0\\n  ]\\n  edge [\\n    source 22\\n    target 23\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 22\\n    target 24\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 25\\n    target 26\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 25\\n    target 27\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 26\\n    target 28\\n    weight 0\\n  ]\\n  edge [\\n    source 30\\n    target 31\\n    weight 0\\n  ]\\n  edge [\\n    source 32\\n    target 33\\n    weight 0\\n  ]\\n  edge [\\n    source 35\\n    target 35\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 36\\n    target 36\\n    weight 3.333333333333333\\n  ]\\n  edge [\\n    source 37\\n    target 37\\n    weight 3.333333333333333\\n  ]\\n]\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('graphs/final_graph.gml') as f:\n",
    "    data = f.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Delete the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete the endpoint after you have used it to save resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
